# -*- coding: utf-8 -*-
"""mcts_llm_planner.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ElBpX8PFn-1k8CMovRXNphjyLLj_3CL2
"""

import random
import math
import matplotlib.pyplot as plt

# Mock LLM function: generates candidate next steps based on current plan
def llm_generate_plan_step(state):
    all_steps = [
        "Wake up", "Eat breakfast", "Exercise", "Check email", "Work on assignment",
        "Attend lecture", "Have lunch", "Study", "Take a break", "Read paper",
        "Write summary", "Dinner", "Call family", "Sleep"
    ]
    # Exclude already planned steps to avoid duplication
    remaining = [step for step in all_steps if step not in state]
    # Return top available steps as candidates
    return remaining[:5] if remaining else ["Sleep"]

# Reward function scores plan quality given constraints
def reward_function(plan, constraints=None):
    reward = 0.1 * len(set(plan))  # Encourage diversity by rewarding unique steps
    penalty = 0
    if constraints:
        # Add positive rewards for satisfied constraints
        reward += sum(1 for c in constraints if c in plan)
        # Penalize for violating simple constraints (example: "Do not plan after 10pm")
        if "Sleep" in plan and plan.index("Sleep") < len(plan) - 2:
            penalty += 1  # Penalty if 'Sleep' too early
    return reward - penalty

# Node class for tree representation in MCTS
class Node:
    def __init__(self, state, parent=None, action=None):
        self.state = state[:]  # List of steps in current plan
        self.parent = parent
        self.action = action
        self.children = []
        self.visits = 0
        self.value = 0.0

# UCT formula to select promising child node balancing exploration and exploitation
def uct_select(node, c=1.4):
    choices = [
        (child.value / (child.visits + 1e-5) + c * math.sqrt(math.log(node.visits + 1) / (child.visits + 1e-5)), child)
        for child in node.children
    ]
    _, selected = max(choices, key=lambda x: x[0])
    return selected

# Expand node by generating children with candidate next steps proposed by LLM
def expand(node, constraints):
    options = llm_generate_plan_step(node.state)
    for action in options:
        next_state = node.state + [action]
        child_node = Node(next_state, parent=node, action=action)
        node.children.append(child_node)
    return node.children

# Simulate rollout (random completion) from current node to estimate reward
def simulate(node, constraints, max_depth=10):
    cur_plan = node.state[:]
    while len(cur_plan) < max_depth:
        step = random.choice(llm_generate_plan_step(cur_plan))
        cur_plan.append(step)
    return reward_function(cur_plan, constraints)

# Backpropagate simulation rewards up the tree
def backup(node, reward):
    while node is not None:
        node.visits += 1
        node.value += reward
        node = node.parent

# Full MCTS-UCT search with logging of rewards over simulations
def mcts_uct_with_logging(root_state, constraints, num_simulations=200, max_depth=10):
    root = Node(root_state)
    logs = []
    for sim in range(num_simulations):
        # Selection phase: traverse tree to select a leaf node
        node = root
        while node.children:
            node = uct_select(node)
        # Expansion: add children if not terminal
        if len(node.state) < max_depth:
            expand(node, constraints)
            if node.children:
                node = random.choice(node.children)
        # Simulation: rollout from leaf to estimate reward
        reward = simulate(node, constraints, max_depth)
        # Backup: propagate reward to ancestors
        backup(node, reward)
        logs.append((sim, reward))
    return root, logs

# Plot reward evolution over time
def plot_simulation_rewards(logs):
    rewards = [r for _, r in logs]
    plt.plot(rewards, color="purple")
    plt.xlabel("Simulation")
    plt.ylabel("Reward")
    plt.title("MCTS Simulation Reward Progression")
    plt.show()

# Example usage
constraints = ["Exercise", "Write summary", "Attend lecture"]

# Run MCTS-UCT with logging enabled
root, logs = mcts_uct_with_logging([], constraints, num_simulations=200, max_depth=10)

# Print best plan found at root based on average values
plans = [child.state for child in root.children]
values = [child.value/(child.visits+1e-5) for child in root.children]
for i, (plan, val) in enumerate(zip(plans, values)):
    print(f"Option {i+1}: Plan = {plan}, Value = {val:.2f}")

best_plan = max(root.children, key=lambda n: n.value/(n.visits+1e-5)).state
print("\nBest plan found by MCTS:")
print(best_plan)

# Show reward progression plot
plot_simulation_rewards(logs)

import random
import math
import matplotlib.pyplot as plt

# Mock LLM function: generates candidate next steps based on current plan
def llm_generate_plan_step(state):
    all_steps = [
        "Wake up", "Eat breakfast", "Exercise", "Check email", "Work on assignment",
        "Attend lecture", "Have lunch", "Study", "Take a break", "Read paper",
        "Write summary", "Dinner", "Call family", "Sleep"
    ]
    remaining = [step for step in all_steps if step not in state]
    return remaining[:5] if remaining else ["Sleep"]

# Reward function: reward for diversity and constraint satisfaction
def reward_function(plan, constraints=None):
    reward = 0.1 * len(set(plan))  # Reward unique steps
    penalty = 0
    if constraints:
        reward += sum(1 for c in constraints if c in plan)
        # Penalty if 'Sleep' is scheduled too early
        if "Sleep" in plan and plan.index("Sleep") < len(plan) - 2:
            penalty += 1
    return reward - penalty

# MCTS Node class
class Node:
    def __init__(self, state, parent=None, action=None):
        self.state = state[:]
        self.parent = parent
        self.action = action
        self.children = []
        self.visits = 0
        self.value = 0.0

# UCT selection
def uct_select(node, c=1.4):
    choices = [
        (child.value / (child.visits + 1e-5) + c * math.sqrt(math.log(node.visits + 1) / (child.visits + 1e-5)), child)
        for child in node.children
    ]
    _, selected = max(choices, key=lambda x: x[0])
    return selected

# Expansion step
def expand(node, constraints):
    options = llm_generate_plan_step(node.state)
    for action in options:
        next_state = node.state + [action]
        child_node = Node(next_state, parent=node, action=action)
        node.children.append(child_node)
    return node.children

# Simulation (rollout)
def simulate(node, constraints, max_depth=10):
    cur_plan = node.state[:]
    while len(cur_plan) < max_depth:
        step = random.choice(llm_generate_plan_step(cur_plan))
        cur_plan.append(step)
    return reward_function(cur_plan, constraints)

# Backpropagation
def backup(node, reward):
    while node is not None:
        node.visits += 1
        node.value += reward
        node = node.parent

# MCTS-UCT with logging
def mcts_uct_with_logging(root_state, constraints, num_simulations=200, max_depth=10):
    root = Node(root_state)
    logs = []
    for sim in range(num_simulations):
        node = root
        while node.children:
            node = uct_select(node)
        if len(node.state) < max_depth:
            expand(node, constraints)
            if node.children:
                node = random.choice(node.children)
        reward = simulate(node, constraints, max_depth)
        backup(node, reward)
        logs.append((sim, reward))
    return root, logs

# Plot simulation rewards
def plot_simulation_rewards(logs):
    rewards = [r for _, r in logs]
    plt.plot(rewards, color="purple")
    plt.xlabel("Simulation")
    plt.ylabel("Reward")
    plt.title("MCTS Simulation Reward Progression")
    plt.show()

# Function to follow best path (greedy) from root for a full sequential plan
def get_full_best_plan(root):
    plan = []
    node = root
    while node.children:
        # pick child with best average value
        node = max(node.children, key=lambda n: n.value/(n.visits+1e-5))
        plan = node.state[:]
    return plan

# Example usage
constraints = ["Exercise", "Write summary", "Attend lecture"]

root, logs = mcts_uct_with_logging([], constraints, num_simulations=200, max_depth=8)

# Print the value of each top-level root action (single-step)
plans = [child.state for child in root.children]
values = [child.value/(child.visits+1e-5) for child in root.children]
for i, (plan, val) in enumerate(zip(plans, values)):
    print(f"Option {i+1}: Plan = {plan}, Value = {val:.2f}")

# Full best plan: follow best child at each step until leaf or max depth
full_plan = get_full_best_plan(root)
print("\nFull best plan found by MCTS:")
print(full_plan)

plot_simulation_rewards(logs)